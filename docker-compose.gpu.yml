# docker compose -f docker-compose.gpu.yml --profile all down --remove-orphans
# docker compose -f docker-compose.gpu.yml --profile cloud up -d
# docker compose -f docker-compose.gpu.yml --profile ingest up -d
# docker compose -f docker-compose.gpu.yml --profile audio_config up -d
# docker compose -f docker-compose.gpu.yml --profile postgres up -d
# docker compose -f docker-compose.gpu.yml --profile all_postgres up -d
# docker compose -f docker-compose.gpu.yml --profile all_postgres_audio up -d
# docker compose -f docker-compose.gpu.yml --profile llm_processor up -d
# docker compose -f docker-compose.gpu.yml --profile text2speech_processor up -d
# docker compose -f docker-compose.gpu.yml --profile speech2text_processor up -d
# docker compose -f docker-compose.gpu.yml --profile baai_processor up -d
# docker compose -f docker-compose.gpu.yml --profile chunker_processor up -d
# docker compose -f docker-compose.gpu.yml logs -f milvus
# docker compose -f docker-compose.gpu.yml logs -f postgres
# docker compose -f docker-compose.gpu.yml logs -f llm_processor
# docker compose -f docker-compose.gpu.yml logs -f baai_processor
# docker compose -f docker-compose.gpu.yml logs -f chunker_processor
# docker compose -f docker-compose.gpu.yml logs -f text2speech_processor
# docker compose -f docker-compose.gpu.yml logs -f speech2text_processor

# docker exec -it ca5f7fd95155 /bin/sh
# apt install postgresql-16-pgvector
# ssh root@localhost -p 2031
# chunker
# ssh root@localhost -p 2242
# speech to text
# ssh root@localhost -p 2227
# text to speech
# ssh root@localhost -p 2243
# llm
# ssh root@localhost -p 2032

networks:
  network_ejflab:
    name: network_ejflab
    driver: bridge
    #driver_opts:
    #  com.docker.network.driver.mtu: 1500
    #ipam:
    #  config:
    #    - subnet: 10.2.0.0/28
    enable_ipv6: false

volumes:
  postgres:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}/postgres/data"
  flowcharts:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}/flowcharts"
  milvus_volume:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}/milvus/volume"
  minio_data:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}/minio/data"
  imageia:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}"
  movies:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}/movies"

services:
  milvus:
    profiles: ["all", "all_milvus"]
    image: milvusdb/milvus:v2.4.5
    container_name: milvus-standalone
    security_opt:
      - seccomp:unconfined
    environment:
      - ETCD_USE_EMBED=true
      - ETCD_DATA_DIR=/var/lib/milvus/etcd
      - ETCD_CONFIG_PATH=/milvus/configs/embedEtcd.yaml
      - COMMON_STORAGETYPE=local
    volumes:
      - "milvus_volume:/var/lib/milvus"
      - "${WORKSPACE}/milvus/configs/embedEtcd.yaml:/milvus/configs/embedEtcd.yaml"
      - "${WORKSPACE}/milvus/configs/user.yaml:/milvus/configs/user.yaml"
    ports:
      - 19530:19530
      - 9091:9091
      - 2379:2379
    healthcheck:
      test: curl -f http://localhost:9091/healthz
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 20
    command: milvus run standalone 1> /dev/null
    networks:
      network_ejflab:
        aliases:
          - milvus

  postgres:
    profiles: [ "all", "postgres", "all_postgres", "all_postgres_audio", "cloud", "ingest", "audio_config" ]
    image: ${IMAGE_POSTGRES}
    ports:
      - 5432:5432
    volumes:
      - postgres:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    networks:
      network_ejflab:
        aliases:
          - postgres

  llm_processor:
    profiles: [ "all", "llm_processor", "all_postgres", "all_postgres_audio" ]
    image: ${IMAGE_LLM_PROCESSOR}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                  - gpu

    environment:
      PYTHONUNBUFFERED: 1
      PROCESSOR_UID: "llm"
      ROOM: ${ROOM}
      CHANNEL: ${CHANNEL}
      CHANNEL_SERVERS: "post"
      SERVER_WS: ${SERVER_WS}
      SERVER_POST_URL: ${SERVER_POST_URL}
      SERVER_PUB_SUB_URL: ${SERVER_PUB_SUB_URL}
      PORT: "8092"
      PYANNOTE_TOKEN: ${PYANNOTE_TOKEN}
      MILVUS_URI: ${MILVUS_URI}
      MODEL: "Meta-Llama-3-8B-Instruct.Q4_0.gguf"
      #MODEL: "orca-mini-3b-gguf2-q4_0.gguf"
      #MODEL: "all-MiniLM-L6-v2.gguf2.f16.gguf"
    ports:
      - "2032:22"
      - "8092:8092"
    volumes:
      - "imageia:/tmp/imageia"
    command: sh /tmp/imageia/processor-pyclient/run.sh llm.py cpu
    networks:
      network_ejflab:
        aliases:
          - llm_processor

  baai_processor:
    profiles: [ "all", "baai_processor", "all_postgres", "all_postgres_audio", "cloud", "ingest" ]
    image: ${IMAGE_BAAI_PROCESSOR}
    environment:
      PYTHONUNBUFFERED: 1
      PROCESSOR_UID: "baai"
      ROOM: ${ROOM}
      CHANNEL: ${CHANNEL}
      CHANNEL_SERVERS: "post"
      SERVER_WS: ${SERVER_WS}
      SERVER_POST_URL: ${SERVER_POST_URL}
      SERVER_PUB_SUB_URL: ${SERVER_PUB_SUB_URL}
      PORT: "8097"
      TF_CPP_MAX_VLOG_LEVEL: "0"
      LD_LIBRARY_PATH: "/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib/:/usr/local/cuda/lib64/:/usr/lib/x86_64-linux-gnu/:/usr/local/lib/python3.8/dist-packages/tensorrt/"
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                  - gpu
    ports:
      - "2240:22"
      - "8097:8097"
    volumes:
      - "imageia:/tmp/imageia"
    networks:
      network_ejflab:
        aliases:
          - baai_processor
    #depends_on:
    #  milvus:
    #    condition: service_healthy
    command: sh /tmp/imageia/processor-pyclient/run.sh baai_processor.py cpu

  chunker_processor:
    profiles: [ "all", "chunker_processor", "all_postgres", "all_postgres_audio", "cloud", "ingest" ]
    image: ${IMAGE_CHUNKER_PROCESSOR}
    environment:
      PYTHONUNBUFFERED: 1
      PROCESSOR_UID: "chunker"
      ROOM: ${ROOM}
      CHANNEL: ${CHANNEL}
      CHANNEL_SERVERS: "post"
      SERVER_WS: ${SERVER_WS}
      SERVER_POST_URL: ${SERVER_POST_URL}
      SERVER_PUB_SUB_URL: ${SERVER_PUB_SUB_URL}
      PORT: "8098"
      TF_CPP_MAX_VLOG_LEVEL: "0"
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
    ports:
      - "2242:22"
      - "8098:8098"
    volumes:
      - "imageia:/tmp/imageia"
    networks:
      network_ejflab:
        aliases:
          - chunker_processor
    command: sh /tmp/imageia/processor-pyclient/run.sh chunker_processor.py cpu

  speech2text_processor:
    profiles: [ "all", "all_postgres_audio", "speech2text_processor", "audio_config" ]
    image: ${IMAGE_SPEECH_TO_TEXT_PROCESSOR}
    environment:
      PYTHONUNBUFFERED: 1
      PROCESSOR_UID: "speechToText"
      ROOM: ${ROOM}
      CHANNEL: ${CHANNEL}
      CHANNEL_SERVERS: ${CHANNEL_SERVERS}
      SERVER_WS: ${SERVER_WS}
      SERVER_POST_URL: ${SERVER_POST_URL}
      SERVER_PUB_SUB_URL: ${SERVER_PUB_SUB_URL}
      PORT: "8087"
      TF_CPP_MAX_VLOG_LEVEL: "0"
      # tiny, base, small, medium, large, large-v1, large-v2, large-v3
      # medium cause OutOfMemoryError in local
      MODEL: "small"
      LD_LIBRARY_PATH: "/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib/:/usr/local/cuda/lib64/:/usr/lib/x86_64-linux-gnu/:/usr/local/lib/python3.8/dist-packages/tensorrt/"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                  - gpu
    ports:
      - "2227:22"
      - "8087:8087"
    volumes:
      - "imageia:/tmp/imageia"
    networks:
      network_ejflab:
        aliases:
          - speech2text_processor
    command: sh /tmp/imageia/processor-pyclient/run.sh speech2text_processor.py gpu

  text2speech_processor:
    profiles: [ "all", "text2speech_processor", "audio_config" ]
    image: ${IMAGE_TEXT_TO_SPEECH}
    environment:
      PYTHONUNBUFFERED: 1
      PROCESSOR_UID: "TextTospeech"
      ROOM: ${ROOM}
      CHANNEL: ${CHANNEL}
      CHANNEL_SERVERS: ${CHANNEL_SERVERS}
      SERVER_WS: ${SERVER_WS}
      SERVER_POST_URL: ${SERVER_POST_URL}
      SERVER_PUB_SUB_URL: ${SERVER_PUB_SUB_URL}
      PORT: "8099"
      TF_CPP_MAX_VLOG_LEVEL: "0"
    ports:
      - "2243:22"
      - "8099:8099"
    volumes:
      - "imageia:/tmp/imageia"
    networks:
      network_ejflab:
        aliases:
          - text2speech_processor
    command: sh /tmp/imageia/processor-pyclient/run.sh text2speech_processor.py cpu