# docker compose -f docker-compose.gpu.yml --profile all down --remove-orphans
# docker compose -f docker-compose.gpu.yml --profile all up -d
# docker compose -f docker-compose.gpu.yml --profile all_milvus up -d
# docker compose -f docker-compose.gpu.yml --profile all_postgres up -d
# docker compose -f docker-compose.gpu.yml --profile llm_processor up -d
# docker compose -f docker-compose.gpu.yml logs -f milvus
# docker compose -f docker-compose.gpu.yml logs -f postgres
# docker compose -f docker-compose.gpu.yml logs -f llm_processor
# docker compose -f docker-compose.gpu.yml logs -f baai_processor

# docker exec -it ca5f7fd95155 /bin/sh
# apt install postgresql-16-pgvector
# ssh root@localhost -p 2031

networks:
  network_ejflab:
    name: network_ejflab
    driver: bridge
    #driver_opts:
    #  com.docker.network.driver.mtu: 1500
    #ipam:
    #  config:
    #    - subnet: 10.2.0.0/28
    enable_ipv6: false

volumes:
  postgres:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}/postgres/data"
  flowcharts:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}/flowcharts"
  milvus_volume:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}/milvus/volume"
  minio_data:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}/minio/data"
  imageia:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}"
  movies:
    driver: local
    driver_opts:
      o: bind
      type: none
      device: "${WORKSPACE}/movies"

services:
  milvus:
    profiles: ["all", "all_milvus"]
    image: milvusdb/milvus:v2.4.5
    container_name: milvus-standalone
    security_opt:
      - seccomp:unconfined
    environment:
      - ETCD_USE_EMBED=true
      - ETCD_DATA_DIR=/var/lib/milvus/etcd
      - ETCD_CONFIG_PATH=/milvus/configs/embedEtcd.yaml
      - COMMON_STORAGETYPE=local
    volumes:
      - "milvus_volume:/var/lib/milvus"
      - "${WORKSPACE}/milvus/configs/embedEtcd.yaml:/milvus/configs/embedEtcd.yaml"
      - "${WORKSPACE}/milvus/configs/user.yaml:/milvus/configs/user.yaml"
    ports:
      - 19530:19530
      - 9091:9091
      - 2379:2379
    healthcheck:
      test: curl -f http://localhost:9091/healthz
      interval: 30s
      start_period: 90s
      timeout: 20s
      retries: 20
    command: milvus run standalone 1> /dev/null
    networks:
      network_ejflab:
        aliases:
          - milvus

  postgres:
    profiles: [ "all", "all_postgres" ]
    image: ${IMAGE_POSTGRES}
    ports:
      - 5432:5432
    volumes:
      - postgres:/var/lib/postgresql/data
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
    networks:
      network_ejflab:
        aliases:
          - postgres

  llm_processor:
    profiles: [ "all", "llm_processor", "all_milvus", "all_postgres" ]
    image: ${IMAGE_LLM_PROCESSOR}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                  - gpu

    environment:
      PYTHONUNBUFFERED: 1
      PROCESSOR_UID: "llm"
      ROOM: ${ROOM}
      CHANNEL: ${CHANNEL}
      CHANNEL_SERVERS: "post"
      SERVER_WS: ${SERVER_WS}
      SERVER_POST_URL: ${SERVER_POST_URL}
      SERVER_PUB_SUB_URL: ${SERVER_PUB_SUB_URL}
      PORT: "8092"
      PYANNOTE_TOKEN: ${PYANNOTE_TOKEN}
      MILVUS_URI: ${MILVUS_URI}
      MODEL: "Meta-Llama-3-8B-Instruct.Q4_0.gguf"
      #MODEL: "orca-mini-3b-gguf2-q4_0.gguf"
      #MODEL: "all-MiniLM-L6-v2.gguf2.f16.gguf"
    ports:
      - "2032:22"
      - "8092:8092"
    volumes:
      - "imageia:/tmp/imageia"
    command: sh /tmp/imageia/processor-pyclient/run.sh llm.py cpu
    networks:
      network_ejflab:
        aliases:
          - llm_processor

  baai_processor:
    profiles: [ "all", "all_postgres" ]
    image: ${IMAGE_BAAI_PROCESSOR}
    environment:
      PYTHONUNBUFFERED: 1
      PROCESSOR_UID: "baai"
      ROOM: ${ROOM}
      CHANNEL: ${CHANNEL}
      CHANNEL_SERVERS: "post"
      SERVER_WS: ${SERVER_WS}
      SERVER_POST_URL: ${SERVER_POST_URL}
      SERVER_PUB_SUB_URL: ${SERVER_PUB_SUB_URL}
      PORT: "8097"
      TF_CPP_MAX_VLOG_LEVEL: "0"
      LD_LIBRARY_PATH: "/usr/local/lib/python3.8/dist-packages/nvidia/cuda_runtime/lib/:/usr/local/cuda/lib64/:/usr/lib/x86_64-linux-gnu/:/usr/local/lib/python3.8/dist-packages/tensorrt/"
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities:
                  - gpu
    ports:
      - "2240:22"
      - "8097:8097"
    volumes:
      - "imageia:/tmp/imageia"
    networks:
      network_ejflab:
        aliases:
          - baai_processor
    #depends_on:
    #  milvus:
    #    condition: service_healthy
    command: sh /tmp/imageia/processor-pyclient/run.sh baai_processor.py gpu